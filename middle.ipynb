{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = pd.read_csv('ratings_train.txt', sep = \"\\t\", engine='python')\n",
    "ratings_test = pd.read_csv('ratings_test.txt', sep = \"\\t\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings_train : 150000\n",
      "ratings_test : 50000\n"
     ]
    }
   ],
   "source": [
    "print('ratings_train :',len(ratings_train))\n",
    "print('ratings_test :',len(ratings_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146182, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train['document'].nunique(), ratings_train['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train.drop_duplicates(subset=['document'], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings_train : 146183\n"
     ]
    }
   ],
   "source": [
    "print('ratings_train :',len(ratings_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  count\n",
      "0      0  73342\n",
      "1      1  72841\n"
     ]
    }
   ],
   "source": [
    "print(ratings_train.groupby('label').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nan값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(ratings_train.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0\n",
      "document    1\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(ratings_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25857</th>\n",
       "      <td>2172111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id document  label\n",
       "25857  2172111      NaN      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train.loc[ratings_train.document.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "ratings_train = ratings_train.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(ratings_train.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146182\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글과 공백을 제외한 것 모두 제거\n",
    "### 1) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train['document'] = ratings_train['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id            0\n",
      "document    391\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ratings_train['document'].replace('', np.nan, inplace=True)\n",
    "print(ratings_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145791\n"
     ]
    }
   ],
   "source": [
    "ratings_train = ratings_train.dropna(how = 'any')\n",
    "print(len(ratings_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>4221289</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>9509970</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>10147571</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>5831045</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>7246718</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148549</th>\n",
       "      <td>9715918</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148566</th>\n",
       "      <td>10110521</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149309</th>\n",
       "      <td>6715725</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149630</th>\n",
       "      <td>3508604</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149773</th>\n",
       "      <td>9233162</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     document  label\n",
       "404      4221289                   0\n",
       "412      9509970                   1\n",
       "470     10147571                   1\n",
       "1312     5831045                   0\n",
       "1549     7246718                   1\n",
       "...          ...          ...    ...\n",
       "148549   9715918                   1\n",
       "148566  10110521                   0\n",
       "149309   6715725                   1\n",
       "149630   3508604                   0\n",
       "149773   9233162                   0\n",
       "\n",
       "[398 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train[ratings_train['document'].str.isspace()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = ratings_train.drop(ratings_train[ratings_train['document'].str.isspace()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings_test : 48995\n"
     ]
    }
   ],
   "source": [
    "ratings_test.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "ratings_test['document'] = ratings_test['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "ratings_test['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "ratings_test = ratings_test.dropna(how='any') # Null 값 제거\n",
    "print('ratings_test :',len(ratings_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_test = ratings_test.drop(ratings_test[ratings_test['document'].str.isspace()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings_test : 48852\n"
     ]
    }
   ],
   "source": [
    "print('ratings_test :',len(ratings_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords 제거 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sen in ratings_train['document']:\n",
    "    X = []\n",
    "    X = okt.morphs(sen, stem=True) # 토큰화\n",
    "    X_train.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for sen in ratings_test['document']:\n",
    "    X = []\n",
    "    X = okt.morphs(sen, stem=True) # 토큰화\n",
    "    X_test.append(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나이브베이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data = []\n",
    "for i in X_train:\n",
    "    t = \" \".join(i)\n",
    "    X_train_data.append(t)\n",
    "    \n",
    "X_test_data = []\n",
    "for i in X_test:\n",
    "    t = \" \".join(i)\n",
    "    X_test_data.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "# DTM\n",
    "x_train_cv = cv.fit_transform(X_train_data) # X_train 각 단어의 빈도 수를 기록한다.\n",
    "\n",
    "# TF-IDF matrix \n",
    "# count vectorizer for other tasks, use TFIDFTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tf_train = tfidf_transformer.fit_transform(x_train_cv)\n",
    "\n",
    "x_test_cv = cv.transform(X_test_data) # X_test 각 단어의 빈도 수를 기록한다.\n",
    "\n",
    "tf_test = tfidf_transformer.transform(x_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (145393, 42120)\n",
      "Tfidf_test: (48852, 42120)\n"
     ]
    }
   ],
   "source": [
    "print('Tfidf_train:',tf_train.shape)\n",
    "print('Tfidf_test:',tf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train = tf_train.toarray()\n",
    "tf_test = tf_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = ratings_train.label\n",
    "Y_test = ratings_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.8269057561614673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "model = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "nb = model.fit(tf_train, Y_train)\n",
    "\n",
    "pred = nb.predict(tf_test)\n",
    "print(\"정확도:\", accuracy_score(Y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    ac.append(accuracy_score(Y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(ac)/len(ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    if(value < thres):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "Y_train = np.delete(Y_train, drop_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = 78)\n",
    "X_test = pad_sequences(X_test, maxlen = 78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1938/1939 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8231\n",
      "Epoch 00001: val_acc improved from -inf to 0.84635, saving model to best_model.h5\n",
      "1939/1939 [==============================] - 84s 43ms/step - loss: 0.3907 - acc: 0.8231 - val_loss: 0.3481 - val_acc: 0.8463\n",
      "Epoch 2/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.3259 - acc: 0.8581\n",
      "Epoch 00002: val_acc improved from 0.84635 to 0.85522, saving model to best_model.h5\n",
      "1939/1939 [==============================] - 83s 43ms/step - loss: 0.3259 - acc: 0.8581 - val_loss: 0.3296 - val_acc: 0.8552\n",
      "Epoch 3/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.3015 - acc: 0.8720\n",
      "Epoch 00003: val_acc improved from 0.85522 to 0.85722, saving model to best_model.h5\n",
      "1939/1939 [==============================] - 83s 43ms/step - loss: 0.3015 - acc: 0.8720 - val_loss: 0.3312 - val_acc: 0.8572\n",
      "Epoch 4/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.2823 - acc: 0.8823\n",
      "Epoch 00004: val_acc improved from 0.85722 to 0.86031, saving model to best_model.h5\n",
      "1939/1939 [==============================] - 83s 43ms/step - loss: 0.2823 - acc: 0.8823 - val_loss: 0.3269 - val_acc: 0.8603\n",
      "Epoch 5/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.2669 - acc: 0.8897\n",
      "Epoch 00005: val_acc did not improve from 0.86031\n",
      "1939/1939 [==============================] - 83s 43ms/step - loss: 0.2669 - acc: 0.8897 - val_loss: 0.3209 - val_acc: 0.8602\n",
      "Epoch 6/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.2534 - acc: 0.8966\n",
      "Epoch 00006: val_acc improved from 0.86031 to 0.86035, saving model to best_model.h5\n",
      "1939/1939 [==============================] - 83s 43ms/step - loss: 0.2534 - acc: 0.8966 - val_loss: 0.3283 - val_acc: 0.8603\n",
      "Epoch 7/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.2437 - acc: 0.9004\n",
      "Epoch 00007: val_acc improved from 0.86035 to 0.86268, saving model to best_model.h5\n",
      "1939/1939 [==============================] - 83s 43ms/step - loss: 0.2437 - acc: 0.9004 - val_loss: 0.3289 - val_acc: 0.8627\n",
      "Epoch 8/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.2264 - acc: 0.9091\n",
      "Epoch 00008: val_acc did not improve from 0.86268\n",
      "1939/1939 [==============================] - 83s 43ms/step - loss: 0.2264 - acc: 0.9091 - val_loss: 0.3323 - val_acc: 0.8593\n",
      "Epoch 9/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.2118 - acc: 0.9162\n",
      "Epoch 00009: val_acc did not improve from 0.86268\n",
      "1939/1939 [==============================] - 83s 43ms/step - loss: 0.2118 - acc: 0.9162 - val_loss: 0.3500 - val_acc: 0.8581\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, Y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1527/1527 [==============================] - 15s 10ms/step - loss: 0.3402 - acc: 0.8591\n",
      "1527/1527 [==============================] - 15s 10ms/step - loss: 0.3402 - acc: 0.8591\n",
      "1527/1527 [==============================] - 15s 10ms/step - loss: 0.3402 - acc: 0.8591\n",
      "1527/1527 [==============================] - 15s 10ms/step - loss: 0.3402 - acc: 0.8591\n",
      "1527/1527 [==============================] - 15s 10ms/step - loss: 0.3402 - acc: 0.8591\n"
     ]
    }
   ],
   "source": [
    "ac = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    ac.append(loaded_model.evaluate(X_test, Y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8590641021728516\n"
     ]
    }
   ],
   "source": [
    "print(sum(ac)/len(ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization - stopwords 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나이브베이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = pd.read_csv('stopwords.txt', sep = \"\\t\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sentence in ratings_train['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for sentence in ratings_test['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data = []\n",
    "for i in X_train:\n",
    "    temp = \" \".join(i)\n",
    "    X_train_data.append(temp)\n",
    "    \n",
    "X_test_data = []\n",
    "for i in X_test:\n",
    "    temp = \" \".join(i)\n",
    "    X_test_data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "# DTM\n",
    "x_train_cv = cv.fit_transform(X_train_data) # X_train 각 단어의 빈도 수를 기록한다.\n",
    "\n",
    "# TF-IDF matrix \n",
    "# count vectorizer for other tasks, use TFIDFTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tf_train = tfidf_transformer.fit_transform(x_train_cv)\n",
    "\n",
    "x_test_cv = cv.transform(X_test_data) # X_test 각 단어의 빈도 수를 기록한다.\n",
    "\n",
    "tf_test = tfidf_transformer.transform(x_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train = tf_train.toarray()\n",
    "tf_test = tf_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = ratings_train.label\n",
    "Y_test = ratings_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.8269057561614673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "model = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "nb = model.fit(tf_train, Y_train)\n",
    "\n",
    "pred = nb.predict(tf_test)\n",
    "print(\"정확도:\", accuracy_score(Y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    ac.append(accuracy_score(Y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8269057561614673\n"
     ]
    }
   ],
   "source": [
    "print(sum(ac)/len(ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "Y_train = np.delete(Y_train, drop_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 78\n"
     ]
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = 78)\n",
    "X_test = pad_sequences(X_test, maxlen = 78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.3886 - acc: 0.8236\n",
      "Epoch 00001: val_acc improved from -inf to 0.84463, saving model to best_model.h5\n",
      "1939/1939 [==============================] - 84s 43ms/step - loss: 0.3886 - acc: 0.8236 - val_loss: 0.3497 - val_acc: 0.8446\n",
      "Epoch 2/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.3265 - acc: 0.8581\n",
      "Epoch 00002: val_acc improved from 0.84463 to 0.85515, saving model to best_model.h5\n",
      "1939/1939 [==============================] - 83s 43ms/step - loss: 0.3265 - acc: 0.8581 - val_loss: 0.3366 - val_acc: 0.8552\n",
      "Epoch 3/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.3013 - acc: 0.8719\n",
      "Epoch 00003: val_acc improved from 0.85515 to 0.85962, saving model to best_model.h5\n",
      "1939/1939 [==============================] - 85s 44ms/step - loss: 0.3013 - acc: 0.8719 - val_loss: 0.3257 - val_acc: 0.8596\n",
      "Epoch 4/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.2819 - acc: 0.8820\n",
      "Epoch 00004: val_acc improved from 0.85962 to 0.86141, saving model to best_model.h5\n",
      "1939/1939 [==============================] - 86s 44ms/step - loss: 0.2819 - acc: 0.8820 - val_loss: 0.3222 - val_acc: 0.8614\n",
      "Epoch 5/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.2672 - acc: 0.8894\n",
      "Epoch 00005: val_acc did not improve from 0.86141\n",
      "1939/1939 [==============================] - 86s 44ms/step - loss: 0.2672 - acc: 0.8894 - val_loss: 0.3231 - val_acc: 0.8596\n",
      "Epoch 6/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.2531 - acc: 0.8963\n",
      "Epoch 00006: val_acc improved from 0.86141 to 0.86152, saving model to best_model.h5\n",
      "1939/1939 [==============================] - 86s 44ms/step - loss: 0.2531 - acc: 0.8963 - val_loss: 0.3285 - val_acc: 0.8615\n",
      "Epoch 7/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.2393 - acc: 0.9037\n",
      "Epoch 00007: val_acc did not improve from 0.86152\n",
      "1939/1939 [==============================] - 86s 44ms/step - loss: 0.2393 - acc: 0.9037 - val_loss: 0.3227 - val_acc: 0.8614\n",
      "Epoch 8/15\n",
      "1939/1939 [==============================] - ETA: 0s - loss: 0.2252 - acc: 0.9101\n",
      "Epoch 00008: val_acc did not improve from 0.86152\n",
      "1939/1939 [==============================] - 86s 44ms/step - loss: 0.2252 - acc: 0.9101 - val_loss: 0.3379 - val_acc: 0.8582\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, Y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1527/1527 [==============================] - 16s 10ms/step - loss: 0.3389 - acc: 0.8591\n",
      "\n",
      " 테스트 정확도: 0.8591\n",
      "1527/1527 [==============================] - 16s 10ms/step - loss: 0.3389 - acc: 0.8591\n",
      "\n",
      " 테스트 정확도: 0.8591\n",
      "1527/1527 [==============================] - 16s 10ms/step - loss: 0.3389 - acc: 0.8591\n",
      "\n",
      " 테스트 정확도: 0.8591\n",
      "1527/1527 [==============================] - 16s 10ms/step - loss: 0.3389 - acc: 0.8591\n",
      "\n",
      " 테스트 정확도: 0.8591\n",
      "1527/1527 [==============================] - 16s 10ms/step - loss: 0.3389 - acc: 0.8591\n",
      "\n",
      " 테스트 정확도: 0.8591\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, Y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1527/1527 [==============================] - 16s 10ms/step - loss: 0.3389 - acc: 0.8591\n",
      "1527/1527 [==============================] - 16s 10ms/step - loss: 0.3389 - acc: 0.8591\n",
      "1527/1527 [==============================] - 16s 10ms/step - loss: 0.3389 - acc: 0.8591\n",
      "1527/1527 [==============================] - 16s 10ms/step - loss: 0.3389 - acc: 0.8591\n",
      "1527/1527 [==============================] - 16s 10ms/step - loss: 0.3389 - acc: 0.8591\n"
     ]
    }
   ],
   "source": [
    "ac = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    ac.append(loaded_model.evaluate(X_test, Y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8591050505638123\n"
     ]
    }
   ],
   "source": [
    "print(sum(ac)/len(ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_py37",
   "language": "python",
   "name": "capstone_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
